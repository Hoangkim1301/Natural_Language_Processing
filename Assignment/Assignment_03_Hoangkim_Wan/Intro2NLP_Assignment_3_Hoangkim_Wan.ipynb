{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing: Assignment 3\n",
    "\n",
    "In this exercise we'll practice features extraction using SpaCy\n",
    "\n",
    "\n",
    "- You can only use built-in Python packages, spaCy and Pandas.\n",
    "- Please comment your code\n",
    "- Submissions are due Sunday at 23:59 **only** on Ilias: **Assignmnets >> Student Submissions >> Assignment 3 (Deadline: 24.05.2022, at 23:59)**\n",
    "\n",
    "- Name the file aproppriately: \"Assignment_3_\\<Your_Name\\>.ipynb\" and submit only the Jupyter Notebook file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (2 points)\n",
    "\n",
    "Write a function `find_top_five(my_file_name)` that takes as input a file name, reads its text, converts everything to lower case and returns the five most frequent tokens and their absolute frequency in a Python dictionary. \n",
    "\n",
    "**Note:**\n",
    "\n",
    "You should ignore punctuation and use spaCy for obtaining the tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9355de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 4), ('is', 4), ('the', 4), ('document', 4), ('first', 2)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "def find_top_five(my_file_name):\n",
    "    top_five_tokens = {}\n",
    "    # load english language model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    with open(my_file_name,'r') as file:\n",
    "\n",
    "        # convert all characters to lower case and ignore all the punctuation\n",
    "        text = file.read().lower().translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "        # process the text using Spacy language model\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # count the tokens\n",
    "        token_counts = Counter(token.text for token in doc if token.text.isalnum())\n",
    "\n",
    "    top_five_tokens = token_counts.most_common(5)\n",
    "\n",
    "    return(print(top_five_tokens))\n",
    "\n",
    "\n",
    "find_top_five('corpus.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (2 points)\n",
    "\n",
    "Write a function `extract_proper_nouns(my_file_name)` that takes a file name as input and returns a list containing all proper nouns with more than one token.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "text = \"Honk Kong and Japan are two countries in Asia and New York is the largest city in the world\"\n",
    "\n",
    "return = `[\"New York\", \"Hong Kong\"]` **(Note: it should not return \"Japan\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proper Nouns:\n",
      "['Hong Kong', 'New York']\n",
      "Total Count: 2\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_proper_nouns(file_name):\n",
    "    # Load the English language model in spaCy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Read the contents of the file\n",
    "    with open(file_name, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract proper nouns\n",
    "    proper_nouns = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk) > 1 and chunk.root.pos_ == 'PROPN': #length of probn muss be >1\n",
    "            proper_nouns.append(chunk.text)\n",
    "\n",
    "    # Print the list of proper nouns and the total count\n",
    "    total_count = len(proper_nouns)\n",
    "    print(\"Proper Nouns:\")\n",
    "    print(proper_nouns)\n",
    "    print(\"Total Count:\", total_count)\n",
    "\n",
    "extract_proper_nouns('task02.txt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (3 points)\n",
    "\n",
    "Write a function `common_lemma(my_file_name)` that takes a file name as input and returns a Python dictionary with lemmas as `key` and the `value` that should contain a list with both verbs and nouns sharing the same lemma.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1.\n",
    "text = \"When users google a word or any query, their system internally runs a pipeline in order to process what the person is querying.\"\n",
    "\n",
    "return = `{\"query\": [\"query\", \"querying\"]}`\n",
    "\n",
    "2.\n",
    "text = \"I really loved the movie and show, the movie was showing reality but it showed sometimes nonesense!\"\n",
    "\n",
    "return = `{\"show\": [\"show\", \"showing\", \"showed\"]}` **(Note: it should not return \"movie\" because both \"movie\"s are NOUN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': ['query', 'querying']}\n",
      "{'show': ['show', 'showing', 'showed']}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def common_lemma(my_file_name):\n",
    "    tokens_with_common_lemma = {}\n",
    "\n",
    "    # load english language model and read file\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    with open(my_file_name, 'r') as file:\n",
    "        text = file.read()\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # filter out all tokens that are not noun or verb\n",
    "        for token in doc:\n",
    "            if token.pos_ in {'NOUN','VERB'}:\n",
    "                # lemmatize the chosen token\n",
    "                original_word = token.lemma_\n",
    "                # if the dictionary contains the lemmatized word and the original word is not added yet, it will be\n",
    "                # added to the list\n",
    "                if tokens_with_common_lemma.keys().__contains__(original_word):\n",
    "                    if not tokens_with_common_lemma[original_word].__contains__(token.text):\n",
    "                        tokens_with_common_lemma[original_word].append(token.text)\n",
    "                else:\n",
    "                # or a new list will be created\n",
    "                    tokens_with_common_lemma[original_word] = [token.text]\n",
    "\n",
    "    # filter out all the pairs that contains only one token, that means it only contains a verb or a noun\n",
    "    tokens_with_common_lemma = {key:value for key,value in tokens_with_common_lemma.items() if len(value) > 1}\n",
    "\n",
    "    return(print(tokens_with_common_lemma))\n",
    "\n",
    "\n",
    "common_lemma(\"task03_1.txt\")\n",
    "common_lemma(\"task03_2.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (3 points)\n",
    "\n",
    "Write a function `token_matcher(string)` that takes the following text as input and prints a list of all the engineering courses mentioned in it + the total number of engineering courses.\n",
    "\n",
    "**Hint:** You should use `Matcher` in SpaCy and define a pattern for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering Courses:\n",
      "['chemical engineering', 'chemical reaction engineering', 'process engineering', 'civil engineering', 'structural engineering', 'architectural engineering', 'transportation engineering', 'geotechnical engineering', 'environmental engineering', 'hydraulic engineering', 'aeronautical engineering', 'computer engineering', 'electrical engineering', 'mechanical engineering']\n",
      "Total Count: 14\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"If you choose to study chemical engineering, you may like to specialize in chemical reaction engineering, plant design, process engineering, process design or transport phenomena. Civil engineering is the professional practice of designing and developing infrastructure projects. This can be on a huge scale, such as the development of\n",
    "nationwide transport systems or water supply networks, or on a smaller scale, such as the development of single roads or buildings.\n",
    "specializations of civil engineering include structural engineering, architectural engineering, transportation engineering, geotechnical engineering,\n",
    "environmental engineering and hydraulic engineering. If you study aeronautical engineering, you could specialize in aerodynamics, aeroelasticity, \n",
    "composites analysis, avionics, propulsion and structures and materials. Computer engineering concerns the design and prototyping of computing hardware and software. \n",
    "This subject merges electrical engineering with computer science, oldest and broadest types of engineering, mechanical engineering is concerned with the design,\n",
    "manufacturing and maintenance of mechanical systems.\"\"\"\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "def token_matcher(string):\n",
    "    # Load the English language model in spaCy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(string)\n",
    "\n",
    "    # Initialize the Matcher\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # Define the pattern for engineering courses\n",
    "    pattern = [{'POS': 'ADJ'}, {'LOWER': {'IN': ['engineering', 'science']}}]\n",
    "\n",
    "    # Add the pattern to the Matcher\n",
    "    matcher.add('ENGINEERING_COURSE', [pattern])\n",
    "\n",
    "    # Extract the matched phrases (engineering courses)\n",
    "    engineering_courses = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.text.lower() in engineering_courses : #no duplictae\n",
    "            continue\n",
    "        if len(chunk)>1 and chunk.text.lower().endswith('engineering') : #length of probn muss be >1\n",
    "            engineering_courses.append(chunk.text.lower())\n",
    "\n",
    "\n",
    "    # Print the list of engineering courses and the total count\n",
    "    total_count = len(engineering_courses)\n",
    "    print(\"Engineering Courses:\")\n",
    "    print(engineering_courses)\n",
    "    print(\"Total Count:\", total_count)\n",
    "\n",
    "# Example usage\n",
    "token_matcher(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
