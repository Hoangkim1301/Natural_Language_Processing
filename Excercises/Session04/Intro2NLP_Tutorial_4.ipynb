{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing\n",
    "\n",
    "## Tutorial 4\n",
    "\n",
    "This tutorial will show how to use spaCy to obtain features that we have been extracting using a rule-based approach on pure Python.\n",
    "\n",
    "### spaCy\n",
    "\n",
    "\n",
    "is an open-source library for advanced NLP in Python, which supports a wide variety of languages. One crucial advantage of using spaCy is that it's designed to be integrated into real-world products without serious difficulties.\n",
    "\n",
    "\n",
    "To begin working with spaCy, we need to specify which language class we want to use. Remember that spaCy was created to be used for several languages. It can't assume that we want to use English. We need to specify this explicitly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "If you haven't intalled spaCy, please run the following line in a separate cell\n",
    "\n",
    "`!pip install spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with LibreSSL 2.8.3. See: https://github.com/urllib3/urllib3/issues/2168",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/__init__.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m Config\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcli\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfo\u001b[39;00m \u001b[39mimport\u001b[39;00m info  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mglossary\u001b[39;00m \u001b[39mimport\u001b[39;00m explain  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mabout\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/cli/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# These are the actual functions, NOT the wrapped CLI commands. The CLI commands\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# are registered automatically and won't have to be imported here.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbenchmark_speed\u001b[39;00m \u001b[39mimport\u001b[39;00m benchmark_speed_cli  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdownload\u001b[39;00m \u001b[39mimport\u001b[39;00m download  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minfo\u001b[39;00m \u001b[39mimport\u001b[39;00m info  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpackage\u001b[39;00m \u001b[39mimport\u001b[39;00m package  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/cli/download.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional, Sequence\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwasabi\u001b[39;00m \u001b[39mimport\u001b[39;00m msg\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/__init__.py:43\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mRequests HTTP Library\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m~~~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m:license: Apache 2.0, see LICENSE for more details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib3\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[1;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:38\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39m# fmt: off\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m         \u001b[39mnot\u001b[39;00m ssl\u001b[39m.\u001b[39mOPENSSL_VERSION\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mOpenSSL \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m         \u001b[39mor\u001b[39;00m ssl\u001b[39m.\u001b[39mOPENSSL_VERSION_INFO \u001b[39m<\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m     ):  \u001b[39m# Defensive:\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39murllib3 v2.0 only supports OpenSSL 1.1.1+, currently \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mssl\u001b[39m\u001b[39m'\u001b[39m\u001b[39m module is compiled with \u001b[39m\u001b[39m{\u001b[39;00mssl\u001b[39m.\u001b[39mOPENSSL_VERSION\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSee: https://github.com/urllib3/urllib3/issues/2168\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m     \u001b[39m# fmt: on\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[39m# === NOTE TO REPACKAGERS AND VENDORS ===\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# Please delete this block, this logic is only\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# for urllib3 being distributed via PyPI.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m# See: https://github.com/urllib3/urllib3/issues/2680\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with LibreSSL 2.8.3. See: https://github.com/urllib3/urllib3/issues/2168"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with an example in English. Since we already know how to tokenize a text, let's take a look of how spaCy does this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import English\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "raw = \"Hard to judge whether these sides were good. We were grossed \" \\\n",
    "      \"out by the melted styrofoam and didn't want to eat it for fear of getting sick.\"\n",
    "\n",
    "doc = nlp(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard to judge whether these sides were good. We were grossed out by the melted styrofoam and didn't want to eat it for fear of getting sick.\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's your turn to do the same for the following Spanish text taken from BBC in Spanish.\n",
    "\n",
    "spanish_raw = '¿Es posible \"desconectar\" a un país entero de internet? ' \\\n",
    "              'La respuesta corta es \"sí\".'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "spaCy uses the same syntax as Python for indexing. This way you can address specific tokens in your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_word = doc[-1]\n",
    "first_word = doc[0]\n",
    "print(first_word, last_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties\n",
    "first_word.text, first_word.lemma_, first_word.pos_, first_word.tag_, first_word.dep_, first_word.shape_, first_word.is_alpha, first_word.is_stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every token in our document has some characteristics that are know in spaCy as **lexical attributes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_word.is_digit)\n",
    "print(last_word.is_punct)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents and spans\n",
    "\n",
    "Two tokens or a sequence of them can be referred to as a span. In some NLP tasks, spans are very relevant. For instance, in Question Answering (QA), obtaining the correct span that answers a query is crucial for the task itself. With spaCy, we can also define spans and use their `lexical attributes` in the same way we can do it for a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = doc[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is reserved for you to explore more about lexical attributes on the previous text. \n",
    "# Check this link: https://spacy.io/api/token for more attributes.\n",
    "third_word = doc[2]\n",
    "print(\"Here is a part-of-speech tag:\", third_word.pos_) # Why is it empty?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get a bit deeper into attributes\n",
    "\n",
    "In our last exercise we could play around with probabilities. Working with language requires most of the time math to solve problems. As an example, we can decide if a the word _tweet_ refers to a noun or to a verb by counting. Can you tell why?\n",
    "\n",
    "Knowing the context of a word and counting how often our desired word appears after a verb or after a noun would give us the probability that we are searching for."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we include statistics in spaCy?\n",
    "\n",
    "The good news is that spaCy provides pre-trained models that we can use depending on our necessities. There is an offer of small, medium and large models for different languages. Having such a model, we can use attributes in context. \n",
    "\n",
    "But what exactly is contained in a pre-trained model? \n",
    "\n",
    "It contains a vocabulary of the words used to train our model, their weights and meta-information useful for spaCy. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and use a small model for English.\n",
    "\n",
    "Please run the following line in a separate cell\n",
    "\n",
    "`!python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we load a spaCy model?\n",
    "\n",
    "Loading the model is as simple as telling spaCy the name of the model to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we already know what to do..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's your turn to create a new document of our English text \n",
    "# and define a span for its last two words excluding the dot.\n",
    "\n",
    "raw = \"Hard to judge whether these sides were good. We were grossed \" \\\n",
    "      \"out by the melted styrofoam and didn't want to eat it for fear of getting sick.\"\n",
    "\n",
    "# new_doc =\n",
    "# last_span = \n",
    "\n",
    "new_doc = nlp(raw)\n",
    "\n",
    "word_two = new_doc[1]\n",
    "last_span = new_doc[-3:-1]\n",
    "print(last_span.text, last_span.start, last_span.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now display part-of-speech tags, dependencies and lemma for them.\n",
    "for token in last_span:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('acomp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure inside spaCy\n",
    "\n",
    "We have seen how to pass raw text to spaCy and process it into lexical features until this point. However, keeping every token for every occurrence in a text is memory expensive. Therefore, spaCy manages everything in a sort of `internal structure`. \n",
    "\n",
    "This structure has three levels or components, the document (**doc**), a vocabulary called **vocab**, and a look-up table called in spaCy the **string store**. The vocab contains token ids stored as **hashes**. From now on, we will call every entry in vocab a **lexeme**. A look-up table indicates which token corresponds to which lexeme."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it look like in terms of code?\n",
    "\n",
    "- A document contains tokens with their lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in last_span:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each object in our vocab is a lexeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = nlp.vocab[last_span[1].text]\n",
    "print(lexeme.text, lexeme.orth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each string representation of a hash id can be searched in the string store and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_string = nlp.vocab.strings[lexeme.orth]\n",
    "searched_hash = nlp.vocab.strings[lexeme.text]\n",
    "\n",
    "print(\"This is my desired string:\", searched_string)\n",
    "print(\"This is my desired hash:\", searched_hash)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displacy\n",
    "\n",
    "You can also look take a look at visualizations, for instance the graphs presented in our slides for today were created with spaCy\n",
    "\n",
    "Let's take a look of an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_string = \"I love apples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_doc = nlp(raw_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(this_doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for specific patterns with Matcher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy provides a `Matcher`, which works similar to regular expressions in Python. The difference is that you can search not only the text, but also other token attributes. In this way we could for example differentiate between _break_ being a verb or a noun and search only for noun appearances.\n",
    "\n",
    "Here, we have examples of searching text, lexical attributes for a specific token and lexical attributes in a more general search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Google Inc. is a company that has a big development in NLP. \"\\\n",
    "       \"When users google for a word or any query, their system internally \" \\\n",
    "       \"runs a pipeline in order to process what the person is querying.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "patterns = [\n",
    "    [{'TEXT': 'Google'}, {'TEXT': 'Inc.'}], \n",
    "    [{'LOWER': 'google'}],\n",
    "    [{'LEMMA': 'query'}, {'IS_PUNCT': True}]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"TEST_PATTERNS\", patterns)\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matches)\n",
    "print(\"Total of matches found:\", len(matches))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what can we do with this output? What does it mean?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Matcher` returns a list of tuples indicating start and end of each found matched span. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a list of found matches\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following what we have seen until now, download a German model and create patterns to find several tokens with more than one ocurrence in the text given in following cells. \n",
    "\n",
    "***Hint:*** Notice that models for other languages were trained on news data instead of web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.examples import sentences\n",
    "raw_german = sentences[0:5]\n",
    "print(raw_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exploration of spaCy\n",
    "https://spacy.io/usage/spacy-101\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A detailed tutorial on spaCy\n",
    "https://www.youtube.com/watch?v=dIUTsFT2MeQ\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
